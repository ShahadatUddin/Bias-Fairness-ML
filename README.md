# Bias-Fairness-ML
Code and resources for the study “Bias in Data, Fairness in Outcomes: Do Biased Datasets Lead to Unfair Model Outcomes in Machine Learning?”

# Overview
This repository contains the code and resources for the study: "_Bias in Data, Fairness in Outcomes: Do Biased Datasets Lead to Unfair Model Outcomes in Machine Learning?_"
  The research investigates whether biased datasets consistently lead to unfair machine learning (ML) predictions and explores how bias mitigation strategies influence fairness outcomes. We evaluate bias using Earth Mover’s Distance (EMD) and fairness using Equalized Odds, Demographic Parity, and Treatment Equality across 22 publicly available datasets.

# Data Source
Please see the supplementary file for the details of the data source and how to download them.

# Code Content
Python scripts for:
    Bias assessment using EMD.
    Fairness evaluation framework.
    Bias mitigation via resampling.

# Citation
Uddin, S., Liang, H., Moni, M.A., Gao, J., & Lu, H. (2026).
Bias in Data, Fairness in Outcomes: Do Biased Datasets Lead to Unfair Model Outcomes in Machine Learning?
